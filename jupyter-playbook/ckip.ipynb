{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af767d-4e63-4df4-98e6-6a1043f070c8",
   "metadata": {},
   "source": [
    "# Ref\n",
    "- https://huggingface.co/ckiplab?sort_models=downloads#models\n",
    "- https://github.com/ckiplab/han-transformers/blob/f08a4573f19a59e3bab4a299f0c6d0a7b72bc5a9/README.md\n",
    "- https://github.com/ckiplab/ckip-transformers\n",
    "- https://github.com/dmmiller612/bert-extractive-summarizer\n",
    "# Source\n",
    "- https://ctext.org/xiyouji/ch1/zh\n",
    "- https://zh.wikipedia.org/wiki/Wiki\n",
    "- https://en.wikipedia.org/wiki/BBC_News\n",
    "# TODO\n",
    "- https://uwsgi-docs.readthedocs.io/en/latest/WSGIquickstart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "066eeaca-bd25-46d4-9cf2-35b5ae29238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckiplab/bert-base-chinese-ner were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ckiplab/bert-base-chinese-ner and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "   BertTokenizerFast,\n",
    "   AutoModelForMaskedLM,\n",
    "   AutoModelForCausalLM,\n",
    "   AutoModelForTokenClassification,\n",
    "   AutoConfig,\n",
    "   AutoModel,\n",
    "   AutoTokenizer,\n",
    ")\n",
    "# Config\n",
    "#custom_config = AutoConfig.from_pretrained('ckiplab/bert-base-han-chinese')\n",
    "#custom_tokenizer = BertTokenizerFast.from_pretrained('ckiplab/bert-base-han-chinese')\n",
    "#custom_tokenizer = AutoTokenizer.from_pretrained('ckiplab/bert-base-han-chinese')\n",
    "#custom_model = AutoModelForCausalLM.from_pretrained('ckiplab/gpt2-base-chinese', config=custom_config)\n",
    "#custom_model = AutoModel.from_pretrained('ckiplab/bert-base-han-chinese', config=custom_config)\n",
    "\n",
    "custom_config = AutoConfig.from_pretrained('bert-base-chinese')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "custom_model = AutoModel.from_pretrained('ckiplab/bert-base-chinese-ner', config=custom_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a46989-0d96-4b00-8c11-25f13c64597b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "custom_config2 = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_config2.output_hidden_states=True\n",
    "custom_tokenizer2 = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "custom_model2 = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=custom_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ebf64f-9c71-4c67-8b42-995c7a202eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from summarizer import Summarizer\n",
    "except:\n",
    "    !pip install bert-extractive-summarizer\n",
    "    from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63160ac8-e6b5-495d-b88b-8aab843549c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wiki（聆聽i/ˈwɪkiː/）是一種可通過瀏覽器存取並由使用者協同編輯其內容的網站。沃德·坎寧安於1995年開發了最初的wiki。他將wiki定義為「一種允許一群使用者用簡單的描述來建立和連接一組網頁的社會計算系統」[1]。 坎寧安後來在別處又寫了這樣的功能，而且這次他還增加了多使用者寫作功能。新功能之一是程式會在每一次任何一張卡片被更改時，自動在「最近更改」卡片上增加一個連往被更改卡片的連結。坎寧安自己常常看「最近更改」卡片，而且還會注意到空白的說明欄位會讓他想要描述一下更改的摘要[7]。 應用\\nWiki\\nwiki在一些需要內容管理系統的企業中得到了廣泛應用[8]、JotSpot和SocialText是創wiki企業應用的先河。wiki可以在高校教育環境中發揮積極的作用，但是直到2006年，wiki應用於教育的案例在全球都比較少。wiki除了被用來建立網站外，也被用作編寫部落格。wiki在中小學教育方面，可以作為學生協助學習的平台。 搜尋\\nwiki提供至少一個標題搜尋，有時是一個全文搜尋。搜尋的可延伸性取決於wiki引擎是否使用一個資料庫。一些wiki（如PmWiki）使用文字檔案[11]。MediaWiki的第一個版本採用文字檔案，但它在21世紀初被李丹尼爾克羅克覆寫成一個資料庫應用程式。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body1 = '''\n",
    "Semafor從公布理念到正式上線，評價兩極，一方面，他們專注經營高成本的國際報導，讓不少人寄予厚望，賈斯汀的前老闆、《大西洋》名譽主席布萊德利（David G. Bradley）就率先投資支持。\n",
    "另一方面，Semafor的定位遭受不少質疑，包括他們口中「2億英語高階人口」，其實是一群利益不同的異質化群體，很難同時滿足需求；有人批評他們為了商業利益，只想服務菁英階層；甚至形容他們有如美國隊長，想要扮演超級英雄解救世界，卻無視身旁被汽車砸到的路人甲，亦即那些消失的地方媒體與讀者。\n",
    "姑不論這些評價是否公允，此刻來看，Semafor的誕生至少凸顯幾個意義：\n",
    "一、媒體信任消蝕確是巨大危機，主流媒體的衰弱、社群訊息的紊亂，助長陰謀論、極端立場與虛假訊息的傳播，Semafor標舉的「報導透明性」，或許無法解決所有問題，卻是展示誠意的開端。\n",
    "二、Semafor透過改造報導文體，回應資訊超載、信任低落的困境。賈斯汀說，他很驚訝「新聞業的核心——報導文體，數百年來並未真正進化。」如今，Axios、Semafor紛紛重塑新聞敘事方式，未來，或許會看到更多新聞文體的創新與改革。\n",
    "三、後社群時代的讀者溝通，已是新聞業當務之急。班在自家專欄指出，社群平台決定遠離新聞，對媒體產業不見得是壞事，「認真的新聞機構必須心無旁騖，將讀者價值與伙伴關係極大化，」不再指望科技巨頭的善意。\n",
    "這是Semafor告訴我們的事，一個國際新聞的美國夢，2名媒體大咖的史密斯任務，他們口中「國際新聞的Netflix」無疑是個美好想像，但也註定遭遇逆風——經濟景氣的逆風，媒體創投的逆風，全球政治分裂的逆風。\n",
    "無論如何，載滿60名水手的船隻已經揚帆，試圖穿梭在險惡洋流間，舉起他們的雙色旗標。\n",
    "'''\n",
    "body2 = '''\n",
    "wiki（聆聽i/ˈwɪkiː/）是一種可通過瀏覽器存取並由使用者協同編輯其內容的網站。沃德·坎寧安於1995年開發了最初的wiki。他將wiki定義為「一種允許一群使用者用簡單的描述來建立和連接一組網頁的社會計算系統」[1]。\n",
    "有些人認爲[2]，wiki系統屬於一種人類知識的網路系統，讓人們可以在web的基礎上對wiki文字進行瀏覽、建立和更改，而且這種建立、更改及發佈的成本遠比HTML文字小。與此同時，wiki系統還支援那些面向社群的協作式寫作，爲協作式寫作提供必要的幫助。最後wiki的寫作者自然構成一個社群，wiki系統爲這個社群提供簡單的交流工具。與其它超文字系統相比，wiki有使用簡便且開放的特點，有助於在一個社群內共享某個領域的知識。 \n",
    "詞源\n",
    "wiki（IPA：[ˈwɪ.kiː]，<WICK-ee>，或[ˈwiː.kiː]，<WEE-kee>）取自夏威夷的Wiki Wiki公車，源自夏威夷語「wiki」，本是「快速」之意[3]。wiki的中文翻譯有維客、圍紀、快紀、共筆和維基等等，其中「維基」一詞是中文維基百科人特別爲維基百科而創，屬於維基媒體的專用術語。隨著「維基」一詞能見度增加，常被泛用爲wiki的主要音譯名。[2]\n",
    "歷史\n",
    "檀香山的Wiki Wiki站牌\n",
    "wiki軟體由軟體設計模式社群開發，用來書寫與討論模式語言。沃德·坎寧安於1995年3月25日成立第一個wiki網站：WikiWikiWeb，用來補充他自己經營的軟體設計模式網站。他發明wiki這個名字以及相關概念，並且實作第一個wiki引擎。坎寧安說自己是根據檀香山的Wiki Wiki公車取名的，「wiki」在夏威夷語爲「快速」之意，這是他到檀香山學會的第一個夏威夷語[來源請求]，故他將「wiki-wiki」作爲「快速」的意思以避免將「這東西」取名爲「快速網」（quick-web）[4][3][5]。\n",
    "坎寧安說，wiki的構想來自他自己在1980年代晚期利用蘋果電腦HyperCard程式作出的一個小功能[6]。HyperCard類似名片整理程式，可用來紀錄人物與相關事物。HyperCard管理許多稱為「卡片」的資料，每張卡片上都可劃分欄位、加上圖片、有樣式的文字或按鈕等等，而且這些內容都可在查閱卡片的同時修改編輯。HyperCard類似於後來的網頁，但是缺乏一些重要特徵。\n",
    "坎寧安認為原來的HyperCard程式十分有用，但創造卡片與卡片之間的連結卻很困難。於是他不用HyperCard程式原本的創造連結功能，而改用「隨選搜尋」的方式自己增添了一個新的連結功能。使用者只要將連結輸入卡片上的一個特殊欄位，而這個欄位每一行都有一個按鈕。按下按鈕時如果卡片已經存在，按鈕就會帶使用者去那張卡片，否則就發出嗶聲，而繼續壓著按鈕不放，程式就會為使用者產生一張卡片。\n",
    "坎寧安向他的朋友展示了這個程式和他自己寫的人事卡片，往往會有人指出卡片之中的內容不太對，他們就可當場利用HyperCard初始的功能修正內容，並利用坎寧安加入的新功能補充連結。\n",
    "坎寧安後來在別處又寫了這樣的功能，而且這次他還增加了多使用者寫作功能。新功能之一是程式會在每一次任何一張卡片被更改時，自動在「最近更改」卡片上增加一個連往被更改卡片的連結。坎寧安自己常常看「最近更改」卡片，而且還會注意到空白的說明欄位會讓他想要描述一下更改的摘要[7]。\n",
    "特徵\n",
    "奧德·坎寧安和波·路夫（Bo Leuf）在《Wiki之道——網上快捷合作》一書中描述wiki概念的幾個本質特徵：\n",
    "wiki允許任何使用者在wiki網站內剪輯任何頁面或新建頁面，不需要任何額外的附加元件，只需透過普通的網頁瀏覽器即可。\n",
    "編輯wiki頁面\n",
    "wiki中使用者使用很多方式來編輯。通常需要透過文字標記式語言。\n",
    "應用\n",
    "Wiki\n",
    "wiki在一些需要內容管理系統的企業中得到了廣泛應用[8]、JotSpot和SocialText是創wiki企業應用的先河。wiki可以在高校教育環境中發揮積極的作用，但是直到2006年，wiki應用於教育的案例在全球都比較少。wiki除了被用來建立網站外，也被用作編寫部落格。wiki在中小學教育方面，可以作為學生協助學習的平台。\n",
    "實施\n",
    "wiki軟體是執行wiki的群組軟體之統稱，允許使用常見的Web瀏覽器建立和修改網頁，被作為應用程式伺服器在多個網頁伺服器上運作。\n",
    "導覽\n",
    "在大多數頁面的文字，通常有大量的超文字連結到其他網頁。大多數wiki有一個反向的功能，它顯示所有連結到一個給定頁面的頁面。\n",
    "認可與安全\n",
    "歷史比較報告加亮顯示頁面不同版本間的變化。\n",
    "控制更改\n",
    "Wikipedia's W.svg  「最近更改」重新導向至此。關於維基百科的相關幫助頁面，詳見「Help:最近更改」。關於維基百科的相關特殊頁面，詳見「Special:最近更改」。\n",
    "wiki的基本設計理念是，與其避免人們犯錯，倒不如讓人們更方便地修正錯誤。因此，wiki固然相當開放，但它有一個有助檢驗最近新增頁面正確性的功能。幾乎每一個wiki網站都有的最突出的功能，就是「最近修訂」頁面——一個列出最近修訂的特殊頁面，或是一個在特定時間範圍內所做修改的列表。[9]:20一些wiki可以對此清單作出過濾，篩去小修改或利用自動指令碼所做之修改（所謂「機器人」）。[9]:54 大部分wiki網站的頁面編修紀錄頁都擁有以下功能：可檢視過去的修訂版本，亦可在任何兩個修訂版本之間進行差異對比。編輯者可以利用修訂歷史瀏覽並且恢復此條目的前一版本；顯示差異功能則能讓編輯者更容易決定是否有必要做此更改。一個普通的wiki使用者可以在「最近修訂」頁面瀏覽差異、查閱歷史、甚至恢復到先前的版本。這個過程基本上是很流暢的，具體細節則要看用的是哪款wiki軟體。[9]:178\n",
    "為了避免人們做出差劣的編輯，有些wiki引擎可以對內容編輯權限進行各種程度的限制，以確保一篇或一組條目的品質。當有人修改某個條目時，願意維護該頁面的使用者（們）會收到通知，讓他/她得以馬上對新編輯進行審查。[9]:109\n",
    "有些wiki會提供「巡邏校訂」（patrolled revisions），讓有權限的編輯者在正當（非破壞）的修訂上做標記。[10]而「標記校訂」（flagged revisions）則是讓普通使用者無法看見尚未通過評審的修訂。\n",
    "搜尋\n",
    "wiki提供至少一個標題搜尋，有時是一個全文搜尋。搜尋的可延伸性取決於wiki引擎是否使用一個資料庫。一些wiki（如PmWiki）使用文字檔案[11]。MediaWiki的第一個版本採用文字檔案，但它在21世紀初被李丹尼爾克羅克覆寫成一個資料庫應用程式。\n",
    "另外有時可以對wiki使用Google等外部搜尋引擎的限定網域搜尋功能以獲得更精確或更全面的結果。\n",
    "規則\n",
    "為了保證內容的品質，大部分wiki系統或其所建立的社群都有一系列錯綜複雜的策略和指導方針，用以對使用者的使用行為進行有一系列的規則控制。\n",
    "比如維基百科總結成以下五個方面：維基百科是一部自由的百科全書；維基百科代表的是一個中立的觀點；維基百科自由編輯內容；維基百科的編輯者應該以一種尊重和文明的方式互相交流；維基百科沒有一成不變的規章，但不可以自由修改規則。\n",
    "社群\n",
    "有許多的wiki社群是私密的，尤其是企業的wiki。企業的wiki有可能只允許內部員工修改。 \n",
    "'''\n",
    "body3 = '''\n",
    "BBC News is an operational business division[1] of the British Broadcasting Corporation (BBC) responsible for the gathering and broadcasting of news and current affairs in the UK and around the world. The department is the world's largest broadcast news organisation and generates about 120 hours of radio and television output each day, as well as online news coverage.[2][3] The service maintains 50 foreign news bureaus with more than 250 correspondents around the world.[4] Deborah Turness has been the CEO of news and current affairs since September 2022.[5]\n",
    "In 2019, it was reported in an Ofcom report that the BBC spent £136m on news during the period April 2018 to March 2019.[6] BBC News' domestic, global and online news divisions are housed within the largest live newsroom in Europe, in Broadcasting House in central London. Parliamentary coverage is produced and broadcast from studios in London. Through BBC English Regions, the BBC also has regional centres across England and national news centres in Northern Ireland, Scotland and Wales. All nations and English regions produce their own local news programmes and other current affairs and sport programmes.\n",
    "The BBC is a quasi-autonomous corporation authorised by royal charter, making it operationally independent of the government. \n",
    "'''\n",
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "model(body2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d3c40-7f0a-4d7a-8a7f-340a4dd8ff1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
